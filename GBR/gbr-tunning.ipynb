{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8822866,"sourceType":"datasetVersion","datasetId":5307918}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.utils import check_random_state\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import callback\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport optuna\nimport pickle\nimport joblib \nimport os\nfrom sklearn.ensemble import GradientBoostingRegressor\ndef cov2corr(cov):\n    # Derive the correlation matrix from a covariance matrix\n    std = np.sqrt(np.diag(cov))\n    corr = cov/np.outer(std,std)\n    corr[corr<-1], corr[corr>1] = -1,1 #for numerical errors\n    return corr\n\ndef clusterKMeansBase(corr0, maxNumClusters=10, n_init=10, debug=False):\n    corr0[corr0 > 1] = 1\n    dist_matrix = ((1-corr0)/2.)**.5\n    silh_coef_optimal = pd.Series(dtype='float64') #observations matrixs\n    kmeans, stat = None, None\n    maxNumClusters = min(maxNumClusters, int(np.floor(dist_matrix.shape[0]/2)))\n    print(\"maxNumClusters\"+str(maxNumClusters))\n    for init in range(0, n_init):\n    #The [outer] loop repeats the first loop multiple times, thereby obtaining different initializations. Ref: de Prado and Lewis (2018)\n    #DETECTION OF FALSE INVESTMENT STRATEGIES USING UNSUPERVISED LEARNING METHODS\n        for num_clusters in range(2, maxNumClusters+1):\n            #(maxNumClusters + 2 - num_clusters) # go in reverse order to view more sub-optimal solutions\n            kmeans_ = KMeans(n_clusters=num_clusters, n_init=10) #, random_state=3425) #n_jobs=None #n_jobs=None - use all CPUs\n            kmeans_ = kmeans_.fit(dist_matrix)\n            silh_coef = silhouette_samples(dist_matrix, kmeans_.labels_)\n            stat = (silh_coef.mean()/silh_coef.std(), silh_coef_optimal.mean()/silh_coef_optimal.std())\n\n            # If this metric better than the previous set as the optimal number of clusters\n            if np.isnan(stat[1]) or stat[0] > stat[1]:\n                silh_coef_optimal = silh_coef\n                kmeans = kmeans_\n                if debug==True:\n                    print(kmeans)\n                    print(stat)\n                    silhouette_avg = silhouette_score(dist_matrix, kmeans_.labels_)\n                    print(\"For n_clusters =\"+ str(num_clusters)+ \"The average silhouette_score is :\"+ str(silhouette_avg))\n                    print(\"********\")\n\n    newIdx = np.argsort(kmeans.labels_)\n    #print(newIdx)\n\n    corr1 = corr0.iloc[newIdx] #reorder rows\n    corr1 = corr1.iloc[:, newIdx] #reorder columns\n\n    clstrs = {i:corr0.columns[np.where(kmeans.labels_==i)[0]].tolist() for i in np.unique(kmeans.labels_)} #cluster members\n    silh_coef_optimal = pd.Series(silh_coef_optimal, index=dist_matrix.index)\n\n    return corr1, clstrs, silh_coef_optimal\n\ndef makeNewOutputs(corr0, clstrs, clstrs2):\n    clstrsNew, newIdx = {}, []\n    for i in clstrs.keys():\n        clstrsNew[len(clstrsNew.keys())] = list(clstrs[i])\n\n    for i in clstrs2.keys():\n        clstrsNew[len(clstrsNew.keys())] = list(clstrs2[i])\n\n    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]\n    corrNew = corr0.loc[newIdx, newIdx]\n\n    dist = ((1 - corr0) / 2.)**.5\n    kmeans_labels = np.zeros(len(dist.columns))\n    for i in clstrsNew.keys():\n        idxs = [dist.index.get_loc(k) for k in clstrsNew[i]]\n        kmeans_labels[idxs] = i\n\n    silhNew = pd.Series(silhouette_samples(dist, kmeans_labels), index=dist.index)\n\n    return corrNew, clstrsNew, silhNew\n\ndef clusterKMeansTop(corr0: pd.DataFrame, maxNumClusters=None, n_init=10):\n    if maxNumClusters == None:\n        maxNumClusters = corr0.shape[1]-1\n\n    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(maxNumClusters, corr0.shape[1]-1), n_init=10)#n_init)\n    print(\"clstrs length:\"+str(len(clstrs.keys())))\n    print(\"best clustr:\"+str(len(clstrs.keys())))\n    #for i in clstrs.keys():\n    #    print(\"std:\"+str(np.std(silh[clstrs[i]])))\n\n    clusterTstats = {i:np.mean(silh[clstrs[i]])/np.std(silh[clstrs[i]]) for i in clstrs.keys()}\n    tStatMean = np.sum(list(clusterTstats.values()))/len(clusterTstats)\n    redoClusters = [i for i in clusterTstats.keys() if clusterTstats[i] < tStatMean]\n    #print(\"redo cluster:\"+str(redoClusters))\n    if len(redoClusters) <= 2:\n        print(\"If 2 or less clusters have a quality rating less than the average then stop.\")\n        print(\"redoCluster <=1:\"+str(redoClusters)+\" clstrs len:\"+str(len(clstrs.keys())))\n        return corr1, clstrs, silh\n    else:\n        keysRedo = [j for i in redoClusters for j in clstrs[i]]\n        corrTmp = corr0.loc[keysRedo, keysRedo]\n        _, clstrs2, _ = clusterKMeansTop(corrTmp, maxNumClusters=min(maxNumClusters, corrTmp.shape[1]-1), n_init=n_init)\n        print(\"clstrs2.len, stat:\"+str(len(clstrs2.keys())))\n        #Make new outputs, if necessary\n        dict_redo_clstrs = {i:clstrs[i] for i in clstrs.keys() if i not in redoClusters}\n        corrNew, clstrsNew, silhNew = makeNewOutputs(corr0, dict_redo_clstrs, clstrs2)\n        newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]])/np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])\n        if newTstatMean <= tStatMean:\n            print(\"newTstatMean <= tStatMean\"+str(newTstatMean)+ \" (len:newClst)\"+str(len(clstrsNew.keys()))+\" <= \"+str(tStatMean)+ \" (len:Clst)\"+str(len(clstrs.keys())))\n            return corr1, clstrs, silh\n        else:\n            print(\"newTstatMean > tStatMean\"+str(newTstatMean)+ \" (len:newClst)\"+str(len(clstrsNew.keys()))\n                  +\" > \"+str(tStatMean)+ \" (len:Clst)\"+str(len(clstrs.keys())))\n            return corrNew, clstrsNew, silhNew\n            #return corr1, clstrs, silh, stat\n\ndef choose_position(roi, trade_threshold = 0.01):\n    pos =0\n    # Predict position base on change in future\n    if roi > trade_threshold:\n        pos = 1\n    elif roi < -trade_threshold:\n        pos = -1\n    else:\n        pos = 0\n\n    return pos\n\ndef backtest_position_ps(position, price, periods, percentage=0.01):\n    #print(periods)\n    # Shift positions to align with future price changes and handle NaN by filling with 0\n    pos = pd.Series(position, index=pd.Series(price).index).shift(1).fillna(0)\n    pos = pd.Series(pos).rolling(int(periods)).sum() #pos for 10 hour predict\n\n    price_array = pd.Series(price).shift(1).fillna(0)\n\n    pos_diff = pos.diff()\n    fee = pos_diff*price_array*0.05*percentage\n\n    # Calculate price changes over the given periods\n    ch = pd.Series(price) - price_array\n\n    # Calculate total PnL\n    total_pnl = pos*ch - fee\n    return total_pnl\n\ndef calculate_sharpe_ratio(pnl):\n    pnl = np.diff(pnl)\n    std = np.std(pnl) if np.std(pnl) != 0 else 0.001\n    sharpe = np.mean(pnl)/std*np.sqrt(252)\n    return sharpe\n\ndef sharpe_for_vn30f(y_pred, y_price, trade_threshold, fee_perc, periods):\n\n    # Predict position base on change in future\n    pos = [choose_position(roi, trade_threshold) for roi in y_pred]\n    pos = np.array(pos)\n\n    # Calculate PNL\n    pnl = backtest_position_ps(pos, y_price, percentage=fee_perc, periods=periods)\n    pnl = np.cumsum(pnl)\n\n    # Standardalize PNL to date\n    daily_pnl = [pnl.iloc[i] for i in range(0, len(pnl), 241)]\n    daily_pnl = pd.Series(daily_pnl).fillna(0)\n\n    # Calculate Sharpe\n    sharpe = calculate_sharpe_ratio(daily_pnl)\n\n    return pos, pnl, daily_pnl, sharpe\n\ndef calculate_hitrate(pos_predict, pos_true):\n    if len(pos_predict) != len(pos_true):\n        raise ValueError(\"Độ dài của hai mảng không khớp\")\n\n    # Tính số lượng dự đoán đúng (các phần tử tương ứng giống nhau)\n    correct_predictions = np.sum(pos_predict == pos_true)\n\n    # Tính tỷ lệ hit rate\n    hit_rate_value = correct_predictions / len(pos_predict)\n\n    return hit_rate_value\n\ndef scale_data(data):\n    scaler = StandardScaler()\n    data = np.where(np.isinf(data), np.nan, data)\n    data = pd.DataFrame(data)\n    data = data.fillna(0)\n    scaler.fit(data)\n    data=pd.DataFrame(scaler.transform(data), index=data.index, columns=data.columns)\n\n    return data\n\ndef split_data(data):\n    new_part = np.array_split(data, 3)\n\n    # Access each part individually\n    hold_out = new_part[2]\n    train_data = pd.concat([new_part[0], new_part[1]], axis=0)\n\n    return train_data, hold_out\n\ndef split_optuna_data(data):\n        train_data, _ = split_data(data)\n        optuna_data = train_data.drop(['close', 'open','high','low','volume', 'Return'], axis=1)\n        optuna_data = scale_data(optuna_data)\n        X_train, X_valid, y_train, y_valid = train_test_split(optuna_data, train_data['Return'], test_size=0.5, shuffle=False)\n\n        return X_train, X_valid, y_train, y_valid\n\ndata = pd.read_csv('/kaggle/input/gbr-new-200-trial/save_data.csv')\n\ntrain_data, hold_out = split_data(data)\n\nwith open('/kaggle/input/gbr-new-200-trial/top_10_list.pkl', 'rb') as f:\n    selected_columns_cluster = pickle.load(f)\n\nmin_delta = 0.0001\npatience = 30\n\nclass CustomEarlyStopping(callback.TrainingCallback):\n    def __init__(self, min_delta, patience, verbose=False):\n        super().__init__()\n        self.min_delta = min_delta\n        self.patience = patience\n        self.verbose = verbose\n        self.best_score = np.inf\n        self.wait = 0\n        self.stopped_epoch = 0\n\n    def after_iteration(self, model, epoch, evals_log):\n        if not evals_log:\n            return False\n        metric_name = next(iter(evals_log['validation_0']))\n        score = evals_log['validation_0'][metric_name][-1]\n        if score < (self.best_score - self.min_delta):\n            self.best_score = score\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                if self.verbose:\n                    print(f\"\\nStopping. Best score: {self.best_score}\")\n                self.stopped_epoch = epoch\n                return True\n        return False\n\n    def get_best_score(self):\n        return self.best_score\n    \ndef objective_params(trial, X_train, X_valid, y_train, y_valid, y_close):\n    # Define the hyperparameter search space\n    params = {\n#         'max_depth': trial.suggest_int('max_depth', 3, 12),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'n_estimators': 8000,  # does not matter, think of it as max epochs, and we stop the model based on early stopping, so any extremely high number works\n# #         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # can't comment, never played with that\n#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # you dont want to sample less than 50% of your data\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),  # you dont want to sample less than 30% of your features pr boosting round\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_samples_split' : trial.suggest_int('min_samples_split', 2, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n        }\n    trade_threshold  = 0.005\n\n    # Check duplication and skip if it's detected.\n    for t in trial.study.trials:\n        if t.state != optuna.trial.TrialState.COMPLETE:\n            continue\n        if t.params == trial.params:\n            return np.nan #t.values  # Return the previous value without re-evaluating i\n\n    custom_early_stopping_instance = CustomEarlyStopping(min_delta=min_delta, patience=patience, verbose=True)\n\n    # Train the model\n    model = GradientBoostingRegressor(**params)\n#     model.fit(X_train, y_train, callbacks=[custom_early_stopping_instance])\n    model.fit(X_train, y_train)\n\n    y_pred_train = model.predict(X_train)\n    y_pred_valid = model.predict(X_valid)\n\n    pos, pnl, daily_pnl, sharpe_is = sharpe_for_vn30f(y_pred_train, y_close[:len(y_pred_train)], trade_threshold=trade_threshold, fee_perc=0.01, periods=10)\n    _, _, _, sharpe_oos = sharpe_for_vn30f(y_pred_valid, y_close[len(y_pred_train):], trade_threshold=trade_threshold, fee_perc=0.01, periods=10)\n\n    return sharpe_oos, abs((abs(sharpe_is / sharpe_oos))-1)\n\nbest_params_list = []\nfor idx, data_item in enumerate(selected_columns_cluster):\n    train_cols, _ = split_data(data_item)\n    optuna_data = scale_data(train_cols)\n\n    X_train, X_valid, y_train, y_valid = train_test_split(optuna_data,\n                                                            train_data['Return'],\n                                                            test_size=0.5,\n                                                            shuffle=False)\n    study = optuna.create_study(directions=['maximize', 'minimize'])\n\n    unique_trials = 50\n    while unique_trials > len(set(str(t.params) for t in study.trials)):\n        study.optimize(lambda trial: objective_params(trial, X_train, X_valid, y_train, y_valid, train_data['Close']), n_trials=1)\n        study.trials_dataframe().fillna(0).sort_values('values_0').to_csv(f'hypertuning{idx}.csv')\n        joblib.dump(study, f'{unique_trials}hypertuningcluster{idx}.pkl')\n\n    # Retrieve all trials\n    trials = study.trials\n\n    completed_trials = [t for t in study.trials if t.values is not None]\n\n    # Sort trials based on objective values\n    completed_trials.sort(key=lambda trial: trial.values, reverse=True)\n\n    # Select top 1 trials\n    params = completed_trials[0].params\n    best_params_list.append(params)\n\n    model =GradientBoostingRegressor(**params)\n    model.fit(X_train, y_train)\n\n#     model.save_model(f'best_in_cluster_{idx}.json')\n    # Save the model using pickle\n    with open(f'best_in_cluster_{idx}.pkl', 'wb') as f:\n        pickle.dump(model, f)\n\nwith open('best_params_list.pkl', 'wb') as f:\n    pickle.dump(best_params_list, f)","metadata":{"_uuid":"0ca331f7-8cc2-4635-ba7b-00aaedc771d9","_cell_guid":"67d957a4-dee1-44b1-a1cb-47d351f61889","execution":{"iopub.status.busy":"2024-07-01T15:17:54.252966Z","iopub.execute_input":"2024-07-01T15:17:54.253386Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n[I 2024-07-01 15:17:56,298] A new study created in memory with name: no-name-951e25f8-d84f-4c8e-93a7-be829bccfb92\n[I 2024-07-01 15:20:42,518] Trial 0 finished with values: [2.8225573228271434, 0.3381809414025212] and parameters: {'n_estimators': 279, 'learning_rate': 0.033850243321549185, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 9}. \n[I 2024-07-01 15:22:01,542] Trial 1 finished with values: [2.58462048003668, 0.2979714542643449] and parameters: {'n_estimators': 177, 'learning_rate': 0.028472730708744808, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 4}. \n[I 2024-07-01 15:22:42,978] Trial 2 finished with values: [2.4909025688322046, 0.14124887772707284] and parameters: {'n_estimators': 62, 'learning_rate': 0.09949434819383499, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 6}. \n[I 2024-07-01 15:26:02,850] Trial 3 finished with values: [2.35356523865537, 0.051877618182492036] and parameters: {'n_estimators': 298, 'learning_rate': 0.02565958256268431, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 5}. \n[I 2024-07-01 15:26:39,384] Trial 4 finished with values: [2.8392687283360467, 0.6060365092370967] and parameters: {'n_estimators': 123, 'learning_rate': 0.012720724871061, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 10}. \n[I 2024-07-01 15:29:44,081] Trial 5 finished with values: [2.8225573228271434, 0.38670741447904466] and parameters: {'n_estimators': 278, 'learning_rate': 0.019333425250130494, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 8}. \n[I 2024-07-01 15:30:23,017] Trial 6 finished with values: [2.8727828292385666, 0.6134803922745038] and parameters: {'n_estimators': 105, 'learning_rate': 0.03437542877190661, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 8}. \n[I 2024-07-01 15:30:50,767] Trial 7 finished with values: [2.8392687283360467, 0.6060365092370967] and parameters: {'n_estimators': 124, 'learning_rate': 0.019089366490319927, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 10}. \n[I 2024-07-01 15:32:01,639] Trial 8 finished with values: [2.2459896010702987, 0.23825163034868124] and parameters: {'n_estimators': 137, 'learning_rate': 0.04115180711779489, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 5}. \n[I 2024-07-01 15:35:07,745] Trial 9 finished with values: [2.5988797023608945, 0.2847787810955482] and parameters: {'n_estimators': 253, 'learning_rate': 0.03676091553593612, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4}. \n[I 2024-07-01 15:36:45,603] Trial 10 finished with values: [2.2156261349625384, 0.8503760740678139] and parameters: {'n_estimators': 264, 'learning_rate': 0.09427353458790803, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1}. \n[I 2024-07-01 15:38:23,233] Trial 11 finished with values: [2.5760288993632843, 0.10235731213796229] and parameters: {'n_estimators': 165, 'learning_rate': 0.05771783380749681, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 7}. \n[I 2024-07-01 15:40:08,522] Trial 12 finished with values: [2.044732128368239, 0.5006852037462819] and parameters: {'n_estimators': 179, 'learning_rate': 0.04853030586949559, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 2}. \n[I 2024-07-01 15:41:32,363] Trial 13 finished with values: [1.8772192380584518, 0.05578412702620539] and parameters: {'n_estimators': 162, 'learning_rate': 0.02394276699988104, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2}. \n[I 2024-07-01 15:42:33,518] Trial 14 finished with values: [2.517272815168634, 0.25920646715084417] and parameters: {'n_estimators': 164, 'learning_rate': 0.07408153628924517, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 7}. \n[I 2024-07-01 15:44:52,926] Trial 15 finished with values: [1.8016302643780433, 0.6342607274944763] and parameters: {'n_estimators': 236, 'learning_rate': 0.036350393042903356, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 3}. \n[I 2024-07-01 15:45:46,891] Trial 16 finished with values: [2.7625035107525555, 0.5170515101171215] and parameters: {'n_estimators': 81, 'learning_rate': 0.07123194436980941, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 10}. \n[I 2024-07-01 15:46:09,200] Trial 17 finished with values: [2.8727828292385666, 0.6134803922745038] and parameters: {'n_estimators': 99, 'learning_rate': 0.08281696556323184, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 8}. \n[I 2024-07-01 15:47:23,508] Trial 18 finished with values: [2.8727828292385666, 0.5567061591729127] and parameters: {'n_estimators': 199, 'learning_rate': 0.027581328903364538, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 9}. \n[I 2024-07-01 15:48:40,621] Trial 19 finished with values: [1.7306874056697152, 0.5442786353435496] and parameters: {'n_estimators': 174, 'learning_rate': 0.04739641495480242, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 2}. \n[I 2024-07-01 15:49:55,582] Trial 20 finished with values: [1.9789177653167116, 0.8252533010149996] and parameters: {'n_estimators': 145, 'learning_rate': 0.09405003485712106, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 2}. \n[I 2024-07-01 15:50:09,952] Trial 21 finished with values: [2.4761418210946737, 0.5269763686264786] and parameters: {'n_estimators': 64, 'learning_rate': 0.07722538135671324, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. \n[I 2024-07-01 15:52:26,804] Trial 22 finished with values: [2.2606018392674088, 0.6208163964864482] and parameters: {'n_estimators': 186, 'learning_rate': 0.08769763170887279, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}. \n[I 2024-07-01 15:53:11,574] Trial 23 finished with values: [2.8727828292385666, 0.5567061591729127] and parameters: {'n_estimators': 120, 'learning_rate': 0.04448919217529561, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 9}. \n[I 2024-07-01 15:55:31,415] Trial 24 finished with values: [1.1061841602653397, 2.4359433395794783] and parameters: {'n_estimators': 191, 'learning_rate': 0.09788574854285545, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}. \n[I 2024-07-01 15:57:44,725] Trial 25 finished with values: [2.8727828292385666, 0.4163776587697632] and parameters: {'n_estimators': 258, 'learning_rate': 0.022774569684804843, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 8}. \n[I 2024-07-01 16:00:39,261] Trial 26 finished with values: [2.040704603989034, 0.7741560051620815] and parameters: {'n_estimators': 264, 'learning_rate': 0.02847578957011919, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 2}. \n[I 2024-07-01 16:02:10,213] Trial 27 finished with values: [2.8727828292385666, 0.4163776587697632] and parameters: {'n_estimators': 153, 'learning_rate': 0.035155459449763594, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 8}. \n[I 2024-07-01 16:03:08,981] Trial 28 finished with values: [1.7195690311667282, 0.7723216910136792] and parameters: {'n_estimators': 114, 'learning_rate': 0.08053256987912359, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 2}. \n[I 2024-07-01 16:05:46,741] Trial 29 finished with values: [1.9418640602433863, 1.030703104814806] and parameters: {'n_estimators': 268, 'learning_rate': 0.0877581136077733, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 2}. \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}